\documentclass[final]{siamart171218}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{bm}

\setlength{\oddsidemargin}{0.65in}
\setlength{\evensidemargin}{0.65in}

\title{Sketch of the ``alternating SQP'' method for fitting Poisson
  topic models}

\author{Peter Carbonetto\thanks{Dept. of Human Genetics and the Research Computing Center, University of Chicago, Chicago, IL}}

\begin{document}

\maketitle

\section{Some derivations}

Given an $n \times p$ matrix of counts $X$ with individual entries
$x_{ij} \geq 0$, our aim is to fit a Poisson model of the counts,
\begin{align}
p(x) &= \prod_{i=1}^n \prod_{j=1}^p p(x_{ij}) \nonumber \\
     &= \prod_{i=1}^n \prod_{j=1}^p \mathrm{Poisson}(x_{ij}; \lambda_{ij}),
\label{eq:poisson-likelihood}
\end{align}
in which the Poisson rates are given by the mixture $\lambda_{ij} =
\sum_{k=1}^K l_{ik} f_{jk}$. Therefore, the Poisson model is specified
by a $p \times K$ matrix $F$ with entries $f_{ik} \geq 0$ (the
``factors''), and an $n \times K$ matrix $L$ with entries $l_{ik} \geq
0$ (the ``loadings''). Fitting $F$ and $L$ is equivalent to
non-negative matrix factorization with the ``beta-divergence'' cost
function \cite{lee-2001}. It can also be used to recover a
maximum-likelihood estimate for the latent Dirichlet allocation (LDA)
model \cite{blei-2003}. So fitting this model is useful for a wide
range of applications.

The log-likelihood for the Poisson model is
\begin{equation}
\log p(x \,|\, F, L) \propto
  \sum_{i=1}^n \sum_{j=1}^p x_{ij} \log
  ({\textstyle \sum_{k=1}^K l_{ik} f_{jk}})
    - \sum_{i=1}^n \sum_{j=1}^p \sum_{k=1}^K l_{ik} f_{jk},
\label{eq:poisson-log-likelihood}
\end{equation}
where the constant of proportionality is obtained from factorial terms
in the Poisson densities. Our specific aim is to find a $F$ and $L$
that maximizes the log-likelihood \eqref{eq:poisson-log-likelihood};
that is, we would like to solve
\begin{equation}
\begin{array}{ll}
\mbox{minimize} & -\log p(x \,|\, F, L) \\
\mbox{subject to} & F \geq 0, L \geq 0.
\end{array}
\label{eq:problem}
\end{equation}
In the remainder, we derive an efficient approach to
doing this.

Our strategy for solving \eqref{eq:problem} is to alternate between
solving for $F$ with $L$ fixed, and solving for $L$ with $F$
fixed. When solving for $F$ with $L$ fixed, and vice versa, the
problem naturally decomposes into a collection of much smaller
subproblems that are more tractable to solve. All the subproblems are
of the following form:
\begin{equation}
\begin{array}{ll}
\mbox{minimize} & \phi(x; B, w) \\
\mbox{subject to} & \mbox{$x_k \geq 0$ for all $k = 1, \ldots, K$},
\end{array}
\label{eq:subproblem}
\end{equation}
in which the objective function is defined as
\begin{equation}
\phi(x; B, w) =
    - \sum_{i=1}^n w_i \log\big({\textstyle \sum_{k=1}^K b_{ik} x_k}\big)
    + \sum_{i=1}^n \sum_{k=1}^K b_{ik} x_k.
\label{eq:subproblem-objective}
\end{equation}
To see the connection between subproblem \eqref{eq:subproblem} and the
original optimization problem, observe that the negative
log-likelihood can be written as
\begin{equation}
-\log p(x \,|\, F, L) = \sum_{i=1}^n \phi(l_i; F, x_i),
\end{equation}
where $x_i$ is the $i$th row of $X$ and $l_i$ is the $i$th row of $L$,
and it can additionally be written as
\begin{equation}
-\log p(x \,|\, F, L) = \sum_{j=1}^p \phi(f_j; L, x_i),
\end{equation}
in which $x_j$ is the $j$th column of $X$, and $f_j$ is the $j$th row
of $F$. Therefore, when $F$ is unchanging, each row of $L$ can be
optimized separately by solving a problem of the form
\eqref{eq:subproblem}, and when $L$ is fixed, each row of $F$ can be
optimized separately by solving a problem of the form
\eqref{eq:subproblem}.

\bibliographystyle{siamplain}
\bibliography{fast-topics}

\end{document}

